{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import *\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "State of satellite\n",
    "P: power of satellite\n",
    "S1: memory in sensor 1\n",
    "S2: memory in sensor 2\n",
    "S3: memory in sensor 3\n",
    "E: electronics of satellite\n",
    "O: orbital motion of satellite\n",
    "T: time of the day-> ionosphere behavior\n",
    "\n",
    "State Space:\n",
    "S = (P, S1, S2, S3, E, O, T)\n",
    "P = {100, 99, 98, …, 3, 2, 1, 0}\n",
    "S1 = {100, 99, 98, …, 3, 2, 1, 0}\n",
    "S2 = {100, 99, 98, …, 3, 2, 1, 0}\n",
    "S3 = {100, 99, 98, …, 3, 2, 1, 0}\n",
    "E = {0, 1} not working / working\n",
    "O = {0, 1} not in range / in range\n",
    "T = {0, 1, 2, 3, 4} night, dawn, morning, afternoon, dusk\n",
    "\n",
    "Action Space:\n",
    "A = {transmit, not transmit} = {1, 0}\n",
    "\n",
    "Step Function:\n",
    "*If a1= transmit = 1, then:\n",
    "    P(state=s) transitions to a lower value P(state=s’) ---> power discharges\n",
    "    Ex: P=100 -> a1 (transmit) -> P=95\n",
    "    S1(state=s) transitions to a lower value S1(state=s’) ---> free memory in sensor 1 decreases (data transferred)\n",
    "    Ex: S1=100 -> a1 (transmit) -> S1=96.5\n",
    "\n",
    "*If a1= not transmit = 0, then:\n",
    "    P(state=s) transitions to a higher value P(state=s’) ---> power charges\n",
    "    Ex: P=70 -> a1 (not transmit) -> P=73\n",
    "    S1(state=s) remains the same S1(state=s’) ---> free memory in sensor 1 constant (no data transferred)\n",
    "    Ex: S1=70 -> a1 (not transmit) -> S1=70\n",
    "\n",
    "Reward:\n",
    "*If a1=1 (transmit), then:    ---> For each transmission we get reward\n",
    "    reward +=5 \n",
    "*If P(state=s’)>=30, then:    ---> Rewards for maintaining power more than 30%\n",
    "    reward +=2 \n",
    "*If we transmit all the data within the time satellite is within range then:\n",
    "    reward += 5*(remaining time for satellite in range)  --->More rewards for quicker transmission\n",
    "\n",
    "Termination:\n",
    "An action can occur every minute for an hour (when the satellite is within range) or until the sensor memory is full i.e. terminate if:\n",
    "*S1(state=s) = 0 --->Transmission complete\n",
    "*O(state=s) = 0  --->Satellite out of range\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SatelliteEnv(Env):\n",
    "    def __init__(self):\n",
    "        # A = {transmit, not transmit} = {1, 0}\n",
    "        self.action_space = Discrete(2)\n",
    "        # S = (P, S1, S2, S3, E, O, T)\n",
    "        #Note currently only using P,S1,O\n",
    "        self.observation_space = Tuple((Box(0, 100, shape=(2,)),MultiDiscrete([2])))  # (P, S1, S2, S3, E, O, T)\n",
    "        # Set start states\n",
    "        self.state = np.zeros(3)\n",
    "        for i in range(3): \n",
    "            if i<2:\n",
    "                self.state[i] = 100\n",
    "            elif i<6:\n",
    "                self.state[i] = 1\n",
    "        \n",
    "        #Transmission Time 60 mins\n",
    "        self.transmission_time = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        #A = {transmit, not transmit} = {1, 0}\n",
    "        #Update States\n",
    "\n",
    "        if self.state[0]>=5: #no action 1 if P < 5 \n",
    "            if action==1:\n",
    "                self.state[0] += -5   #discharging\n",
    "                self.state[1] += -3 #update free memory\n",
    "        if action==0:\n",
    "            self.state[0] += +3 #charging\n",
    "            \n",
    "        #Check States within bounds\n",
    "        for i in range(2):\n",
    "            if self.state[i]>100:\n",
    "                self.state[i] = 100  \n",
    "            elif self.state[i]< 0:\n",
    "                self.state[i] = 0\n",
    "             \n",
    "        # Reduce transmission time by 1 minute\n",
    "        self.transmission_time -= 1\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = 0\n",
    "        if self.state[0]>=30:\n",
    "            reward += 2\n",
    "        else: \n",
    "            reward += 0\n",
    "        if action==1:\n",
    "            reward += 5\n",
    "        if self.state[1]==0:\n",
    "            reward += 100\n",
    "        \n",
    "        # Check if transmission time is done\n",
    "        if self.transmission_time <= 0:\n",
    "            self.state[2] = 0\n",
    "    \n",
    "        if 0 in self.state[[1,2]]:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset states\n",
    "        self.state = np.zeros(3)\n",
    "        for i in range(3): \n",
    "            if i<2:\n",
    "                self.state[i] = 100 #random.randint(90,100)\n",
    "            else:\n",
    "                self.state[i] = 1\n",
    "        # Reset time\n",
    "        self.transmission_time = 60 \n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SatelliteEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([86.32203, 88.84843], dtype=float32), array([0], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1   Score:266.0   Transmissions:  36   Total Time:  60   Final States(P,S,O):   4.0,   1.0,   0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    a=[]\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        a.append(action)\n",
    "        score+=reward\n",
    "    out_states = ','.join(map('{:6}'.format, n_state[[0,1,2]]))\n",
    "    print('Episode:{0:3}   Score:{1:5,.1f}   Transmissions:{2:4}   Total Time:{3:4}   Final States(P,S,O):{4:5}'.format(episode,score,a.count(1),np.size(a), out_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S_lookup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricky\\AppData\\Local\\Temp/ipykernel_60268/1700741764.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state = np.array([P,S1,O])\n"
     ]
    }
   ],
   "source": [
    "P = np.arange(0,101,1)\n",
    "S1 = np.append([0],np.arange(1,101,3))\n",
    "O = np.arange(2)\n",
    "A = np.arange(2)\n",
    "state = np.array([P,S1,O])\n",
    "N = np.size(state[0])*np.size(state[1])*np.size(state[2])\n",
    "S_lookup = np.zeros([N,3])\n",
    "i = 0\n",
    "\n",
    "for l in range(np.size(state[0])):\n",
    "    for m in range(np.size(state[1])):\n",
    "        for n in range(np.size(state[2])):\n",
    "            S_lookup[i] = [state[0][l],state[1][m],state[2][n]]\n",
    "            i = i + 1\n",
    "S_lookup = S_lookup.astype(int)\n",
    "dims = S_lookup.max (0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating matrices T(s, a, s') and R(s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.arange(0, 7070)\n",
    "A = [0, 1]\n",
    "T = np.zeros([7070, 2, 7070])  # s, a, s'\n",
    "R = np.zeros([7070, 2])\n",
    "gamma = 1\n",
    "dims = S_lookup.max (0)+1\n",
    "\n",
    "# Populate matrices\n",
    "for s in S:\n",
    "    p, s1, o = S_lookup[s]\n",
    "    # action = 0 transition matrix\n",
    "    pp = p + 3\n",
    "    if pp > 100:\n",
    "        pp = 100\n",
    "    s1p = s1\n",
    "    op = o\n",
    "    n_state = np.array([pp, s1p, op])\n",
    "    sp = np.where(np.in1d(np.ravel_multi_index(S_lookup.T,dims),np.ravel_multi_index(n_state.T,dims)))[0][0]\n",
    "    T[s, 0, sp] += 1\n",
    "\n",
    "    # action = 1 transition matrix\n",
    "    if p >= 5:\n",
    "        pp = p - 5\n",
    "        s1p = s1 - 3\n",
    "        if s1p < 0:\n",
    "            s1p = 0\n",
    "        op = o\n",
    "        n_state = np.array([pp, s1p, op])\n",
    "        sp = np.where(np.in1d(np.ravel_multi_index(S_lookup.T,dims),np.ravel_multi_index(n_state.T,dims)))[0][0]\n",
    "        T[s, 1, sp] += 1\n",
    "        \n",
    "    ## Fill Reward matrices\n",
    "    if p >= 30:\n",
    "        R[s,0] += 2\n",
    "        R[s,1] += 2\n",
    "    R[s,1] += 5\n",
    "    if s1 == 0:\n",
    "        R[s,0] += 100\n",
    "        R[s,1] += 100 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([7070, 2])\n",
    "U = np.arange(0, 7070)\n",
    "Policies = []\n",
    "threshold = 0.5\n",
    "#for i in range(100):  # when ready, change to while loop until convergence\n",
    "for s in range(len(S)):\n",
    "    for a in range(len(A)):\n",
    "        Q[s, a] = R[s,a] + (gamma * sum([T[s, a, sp] * U[sp] for sp in S]))\n",
    "    Uold = U\n",
    "    U = np.max(Q, axis=1)\n",
    "    if max(abs(Uold - U)) < threshold:\n",
    "        print('stopped after ' + str(s) + ' loops.' )\n",
    "        break\n",
    "Policy = np.argmax(Q, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1   Score:328.0   Transmissions:  60   Total Time:  60   Final States(P,S,O):   0.0,  40.0,   0.0\n",
      "Episode:  2   Score:328.0   Transmissions:  60   Total Time:  60   Final States(P,S,O):   0.0,  40.0,   0.0\n",
      "Episode:  3   Score:328.0   Transmissions:  60   Total Time:  60   Final States(P,S,O):   0.0,  40.0,   0.0\n",
      "Episode:  4   Score:328.0   Transmissions:  60   Total Time:  60   Final States(P,S,O):   0.0,  40.0,   0.0\n",
      "Episode:  5   Score:328.0   Transmissions:  60   Total Time:  60   Final States(P,S,O):   0.0,  40.0,   0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    a=[]\n",
    "    action = 1\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        #action = env.action_space.sample()\n",
    "        a.append(action)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        p, s1, o = n_state\n",
    "        score+=reward\n",
    "        n_state_3 = np.array([int(p), int(s1), int(o)]) \n",
    "        n_state_idx = np.where(np.in1d(np.ravel_multi_index(S_lookup.T,dims),np.ravel_multi_index(n_state_3.T,dims)))[0][0]\n",
    "        action = Policy[n_state_idx]\n",
    "    out_states = ','.join(map('{:6}'.format, n_state[[0,1,2]]))\n",
    "    print('Episode:{0:3}   Score:{1:5,.1f}   Transmissions:{2:4}   Total Time:{3:4}   Final States(P,S,O):{4:5}'.format(episode, score, a.count(1), np.size(a), out_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
